{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPF6P4y5LyJ9BziivXdMqx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolagojakovic/text-to-image-diffusion_model/blob/main/conditional_diffusion_text_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional Diffusion Model - Educational Implementation**\n",
        "\n",
        "This notebook contains my implementation of a conditional diffusion model, created for educational purposes to understand the core concepts behind modern text-to-image generation systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Dataset:https://www.kaggle.com/datasets/jessicali9530/celeba-dataset/data\n",
        "\n",
        "Papers:\n",
        " - https://arxiv.org/pdf/2006.11239\n",
        " - https://arxiv.org/pdf/2103.00020\n",
        " - https://arxiv.org/pdf/2112.10752\n"
      ],
      "metadata": {
        "id": "V0Rjq5_fcnNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "r5Ewdys_eI6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core PyTorch imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Diffusers library for diffusion models\n",
        "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "# Transformers library for CLIP text encoder\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# Additional utilities\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CelebA\n",
        "import random\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "w2ndM7OzePak"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "vE2Qgl1dgIus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebAWithText(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        self.celeba = CelebA(root=root_dir, split=split, download=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Simple attribute-based text descriptions\n",
        "        self.attr_descriptions = {\n",
        "            5: \"person with bangs\", 8: \"person with black hair\", 9: \"person with blond hair\",\n",
        "            10: \"person with brown hair\", 15: \"person with heavy makeup\", 17: \"male person\",\n",
        "            28: \"person smiling\", 31: \"person wearing earrings\", 33: \"person wearing lipstick\",\n",
        "            39: \"female person\"\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.celeba)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, attributes = self.celeba[idx]\n",
        "\n",
        "        # Generate text description from attributes\n",
        "        active_attrs = [i for i, attr in enumerate(attributes) if attr == 1 and i in self.attr_descriptions]\n",
        "        if active_attrs:\n",
        "            selected_attrs = random.sample(active_attrs, min(2, len(active_attrs)))\n",
        "            text_parts = [self.attr_descriptions[attr] for attr in selected_attrs]\n",
        "            text = \"a photo of \" + \", \".join(text_parts)\n",
        "        else:\n",
        "            text = \"a photo of a person\"\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, text\n",
        "\n",
        "# Your load_batch_data function - enhanced for CelebA\n",
        "def load_batch_data(batch_size=128):\n",
        "    # Setup data transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = CelebAWithText(root_dir=\"./data\", split='train', transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Get one batch\n",
        "    images, text = next(iter(dataloader))\n",
        "    return images, text\n",
        "\n",
        "def augment(images):\n",
        "    # Run augmentations like flipping, brightness/contrast\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "rOqyc_EJg6eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "eTUpv6KbhKdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "- Conditional UNet\n",
        "- Conditional Latent Diffusion Model"
      ],
      "metadata": {
        "id": "Sto3AQj5hg-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models for your training loop\n",
        "def initialize_models():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize VAE\n",
        "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "\n",
        "    # Initialize CLIP model for text encoding\n",
        "    clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    # Initialize UNet model\n",
        "    unet_model = UNet2DConditionModel(\n",
        "        sample_size=32,\n",
        "        in_channels=4,  # VAE latent channels\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"),\n",
        "        up_block_types=(\"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n",
        "        block_out_channels=(64, 128, 256),\n",
        "        cross_attention_dim=512,\n",
        "    )\n",
        "\n",
        "    # Initialize noise scheduler\n",
        "    noise_scheduler = DDPMScheduler(\n",
        "        num_train_timesteps=1000,\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\"\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    vae = vae.to(device)\n",
        "    clip_model = clip_model.to(device)\n",
        "    unet_model = unet_model.to(device)\n",
        "\n",
        "    # Freeze VAE and CLIP\n",
        "    vae.requires_grad_(False)\n",
        "    clip_model.requires_grad_(False)\n",
        "\n",
        "    return vae, clip_model, tokenizer, unet_model, noise_scheduler, device\n",
        "\n",
        "# Enhanced text encoding function\n",
        "def encode_text_with_clip(text, tokenizer, clip_model, device):\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, max_length=77, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        text_embedding = clip_model(tokens.input_ids)[0]\n",
        "    return text_embedding, tokens.attention_mask\n",
        "\n",
        "# Training loop - enhanced with proper setup\n",
        "def train_diffusion_model():\n",
        "    # Initialize everything\n",
        "    vae, clip_model, tokenizer, unet_model, noise_scheduler, device = initialize_models()\n",
        "    optimizer = AdamW(unet_model.parameters(), lr=1e-4)\n",
        "    max_timesteps = noise_scheduler.config.num_train_timesteps\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    batch_size = 4  # Reduced for memory efficiency\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for step in tqdm(range(100)):  # 100 steps per epoch for demo\n",
        "\n",
        "            # Load Data -\n",
        "            images, text = load_batch_data(batch_size=batch_size)\n",
        "            images = images.to(device)\n",
        "            img_shape = images.shape  # [Batch Size, Num Channels, W, H]\n",
        "\n",
        "            # Run augmentations like flipping, brightness/contrast -\n",
        "            augmented_images = augment(images)\n",
        "\n",
        "            # Run Encoder -\n",
        "            with torch.no_grad():\n",
        "                encoded = vae.encode(augmented_images).latent_dist.sample()\n",
        "                encoded = encoded * vae.config.scaling_factor\n",
        "\n",
        "            # Run text encoder - Y\n",
        "            text_embedding, attn_mask = encode_text_with_clip(text, tokenizer, clip_model, device)\n",
        "\n",
        "            # Generate Random Noise for Random timesteps -\n",
        "            timesteps = torch.randint(0, max_timesteps, (len(encoded),), device=device)\n",
        "            noise = torch.randn_like(encoded)\n",
        "\n",
        "            # Add noise to images -\n",
        "            noisy_images = noise_scheduler.add_noise(encoded, noise, timesteps)\n",
        "\n",
        "            # UNet Forward Pass -\n",
        "            prediction = unet_model(noisy_images, timesteps,\n",
        "                                   encoder_hidden_states=text_embedding,\n",
        "                                   encoder_attention_mask=attn_mask,\n",
        "                                   return_dict=False)[0]\n",
        "\n",
        "            # Calculate Loss -\n",
        "            loss = F.mse_loss(prediction, noise)\n",
        "\n",
        "            # Update Weights -\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Save model after each epoch\n",
        "        torch.save(unet_model.state_dict(), f'unet_celeba_epoch_{epoch+1}.pt')\n",
        "        print(f\"Epoch {epoch+1} completed, model saved!\")"
      ],
      "metadata": {
        "id": "lXlUxYyghR2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating\n"
      ],
      "metadata": {
        "id": "e9jMleP-jLYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(prompt=\"a photo of a person with blond hair, smiling\"):\n",
        "    vae, clip_model, tokenizer, unet_model, noise_scheduler, device = initialize_models()\n",
        "\n",
        "    # Load your trained weights (uncomment when you have trained model)\n",
        "    # unet_model.load_state_dict(torch.load('unet_celeba_epoch_5.pt'))\n",
        "\n",
        "    unet_model.eval()\n",
        "\n",
        "    # Encode prompt\n",
        "    text_embedding, attn_mask = encode_text_with_clip([prompt], tokenizer, clip_model, device)\n",
        "\n",
        "    # Generate\n",
        "    noise_scheduler.set_timesteps(50)\n",
        "    latents = torch.randn((1, 4, 32, 32), device=device)\n",
        "\n",
        "    for t in tqdm(noise_scheduler.timesteps):\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet_model(\n",
        "                latents, t.unsqueeze(0),\n",
        "                encoder_hidden_states=text_embedding,\n",
        "                encoder_attention_mask=attn_mask,\n",
        "                return_dict=False\n",
        "            )[0]\n",
        "            latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "    # Decode to image\n",
        "    with torch.no_grad():\n",
        "        latents = latents / vae.config.scaling_factor\n",
        "        image = vae.decode(latents).sample\n",
        "\n",
        "    # Convert to PIL\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(image[0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training with your code structure...\")\n",
        "    train_diffusion_model()\n",
        "\n",
        "    print(\"Generating sample image...\")\n",
        "    sample_image = generate_sample()\n",
        "    sample_image.save(\"sample_output.png\")\n",
        "    print(\"Sample saved as 'sample_output.png'\")"
      ],
      "metadata": {
        "id": "sC822SKijN7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}